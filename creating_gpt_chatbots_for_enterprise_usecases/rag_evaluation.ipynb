{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2036f5a4-b59d-4296-9e82-47aae49853c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de436dcb-99dd-4191-9335-43c8413428ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 08:06:37.541 INFO    streamlit.runtime.secrets: Secrets found in multiple locations: C:\\Users\\joyde\\.streamlit\\secrets.toml, D:\\documents\\github\\webinars\\creating_gpt_chatbots_for_enterprise_usecases\\.streamlit\\secrets.toml. When multiple secret.toml files exist, local secrets will take precedence over global secrets.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "import openai\n",
    "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "openai.api_key = st.secrets.openai_key\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06b9422-81e0-4b66-a13b-298f07cfbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE TEXT AS `Document`'s\n",
    "reader = SimpleDirectoryReader(input_dir=\"./data\", recursive=True)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc7d55-2f2e-416c-8bf8-94e05f67fda5",
   "metadata": {},
   "source": [
    "## Retriever evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9ba494-ee7a-45bc-ba34-839eb43214ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454955b7-edb2-4a0f-94b4-2e8363075c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "vector_index = VectorStoreIndex.from_documents(documents=documents, embed_model=embed_model)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c8959c-5dde-422e-abcd-e151763f76f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "print(f'{len(nodes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19817191-4104-440e-9cae-9ea9a8385667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c5107950-2a6d-46e6-8bfb-468871b05aa8<br>**Similarity:** 0.7106077511023658<br>**Text:** Human Resource Policy Manual Version 1.0   Karvy Financial Services Ltd  \n",
       " \n",
       "This document is a proprietary information of KFSL  and should not be reproduced or altered without requisite p ermissions.  \n",
       " \n",
       "       \n",
       "Confidential   Page 26 of 28  \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       "CONFIRMATION APPRAISAL FORM  \n",
       " \n",
       "Employee Name:  \n",
       "Employee Number:  Date of Joining:  \n",
       "Department:  Location:  \n",
       "Immediate Supervisor:  Due Date for Confirmation:  \n",
       " \n",
       "Comments on employee review:  \n",
       "Please give your assessment of the employee’s performance du ring the probation period  \n",
       "(You are requested to keep in mind that the employee is new to the organization, and focus on \n",
       "whether He/She has demonstrated an ability to understand all aspects of the function he/she is \n",
       "performing, as well as the basic skills a nd behaviors required to perform the role effectively)  \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       " \n",
       "Recommendation  \n",
       "(Please tick your recommendation)  \n",
       " \n",
       "Recommended for confirmation  Recommended for extension of probation f...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 7d551334-eb8f-4d96-a10d-60ef7b9b2fe1<br>**Similarity:** 0.6860221231511127<br>**Text:** Human Resource Policy Manual Version 1.0   Karvy Financial Services Ltd  \n",
       " \n",
       "This document is a proprietary information of KFSL  and should not be reproduced or altered without requisite p ermissions.  \n",
       " \n",
       "       \n",
       "Confidential   Page 11 of 28  \n",
       "Step 2. (D-20) Within 10 days of receipt of the confirmation appraisal form the supervisor should \n",
       "have a formal discussion with the appraisee . This discu ssion should revolve around the \n",
       "appraisee’s performance on KRA for the specific period, any lim itations he/she has in executing  \n",
       "his/her duties etc  \n",
       " \n",
       "Step 3. Post the personal discussion the supervisor and the appraisee should arrive at a \n",
       "consensus on the pe rformance during the last five months. Incase they are not able to arrive at a \n",
       "consensus, the matter has to be referred to the skip level supervisor and HR. The decision of the \n",
       "skip level supervisor and HR shall be final.  \n",
       " \n",
       "Step 4. (D-15) Based on discussio n the supervisor needs to inform HR either on confirmation of \n",
       "services...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"Who is responsible for completing the Confirmation Appraisal Form?\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c094f77e-bf2e-4c74-a4f5-347bfeeefa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# sample_nodes = random.sample(nodes, 5)\n",
    "\n",
    "# qa_dataset = generate_question_context_pairs(\n",
    "#     sample_nodes, llm=judge_llm, num_questions_per_chunk=2\n",
    "# )\n",
    "\n",
    "# print(f'number of queries={len(qa_dataset.queries)}')\n",
    "\n",
    "# qa_dataset.save_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510f1031-ae1d-42da-bff4-12564b9d5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_dataset.save_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef026934-2542-46df-b237-8d3131ba964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa19269d-7a41-4fe9-bba2-ded4f496e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "metrics = [\"mrr\", \"hit_rate\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffe6cf4-9d24-4a86-b80c-dd10b0c270cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the process for employees to avail their mandatory leave, and why is it recommended to submit a tentative leave schedule at the beginning of the year?\n",
      "Metrics: {'mrr': 0.0, 'hit_rate': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[8]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15bb20a-97f0-48f7-9c66-8ff50e0aee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef25deb2-04a2-4788-b554-2d0eb2a525ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_retrieval_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    columns = {\"retrievers\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5464a00-39a3-486f-9ca9-6ce5298e8103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top-2 eval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrievers  hit_rate  mrr\n",
       "0  top-2 eval       0.0  0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_retrieval_results(\"top-2 eval\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3052871-dc08-4089-be5b-86a3d1975733",
   "metadata": {},
   "source": [
    "## LLM evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d594451-ad8d-4b5b-83c9-c689478b7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generator = DatasetGenerator.from_documents(documents)\n",
    "# eval_questions = data_generator.generate_questions_from_nodes(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afda16b8-ac8d-43c4-8ad2-a04c5fad20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "eval_questions_filename = 'eval_questions.json'\n",
    "\n",
    "# with open(eval_questions_filename, \"w\") as fp:\n",
    "#     json.dump(eval_questions, fp)\n",
    "\n",
    "with open(eval_questions_filename, \"r\") as fp:\n",
    "    eval_questions = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97f00c04-bd2b-4b50-aa86-7374cc02b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['What is the file type of the document \"HR_Policy_Manual_KFSLnew.pdf\"?', 'When was the Human Resource Policy Manual Version 1.0 created?']\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_questions))\n",
    "print(eval_questions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b062258-bec0-4586-ae56-a4158c10de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "rag_llm = Ollama(model=\"phi\", request_timeout=300)\n",
    "\n",
    "embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "vector_index = VectorStoreIndex.from_documents(documents=documents, embed_model=embed_model)\n",
    "query_engine = vector_index.as_query_engine(llm=rag_llm)\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=judge_llm)\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=judge_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cceb3391-2311-471b-b40c-9f9a97d5bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_generation(eval_questions, query_engine, relevancy_evaluator, faithfulness_evaluator):\n",
    "    evals = []\n",
    "    for eval_q in tqdm(eval_questions):\n",
    "        import time\n",
    "        time.sleep(30)\n",
    "        response_vector = query_engine.query(eval_q)\n",
    "        relevancy_result = relevancy_evaluator.evaluate_response(query=eval_q, response=response_vector)\n",
    "        faithfulness_result = faithfulness_evaluator.evaluate_response(response=response_vector)\n",
    "        this_df = {\n",
    "            \"Query\": eval_q,\n",
    "            \"Response\": str(response_vector),\n",
    "            \"Source\": (\n",
    "                response_vector.source_nodes[0].node.get_content()[:1000] + \"...\"\n",
    "            ),\n",
    "            \"Relevancy\": relevancy_result.passing,\n",
    "            \"Faithfulness\": faithfulness_result.passing,\n",
    "        }\n",
    "        evals.append(this_df)\n",
    "    eval_df = pd.DataFrame(evals)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f40bfc4-afa1-460f-acff-b4079dccda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▉                                                                                                           | 1/10 [00:52<07:50, 52.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▊                                                                                               | 2/10 [01:54<07:46, 58.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████▍                                                                                  | 3/10 [05:06<13:55, 119.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████▏                                                                      | 4/10 [06:26<10:21, 103.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████▌                                                           | 5/10 [07:34<07:33, 90.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████▍                                               | 6/10 [08:38<05:26, 81.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████████████████████▎                                   | 7/10 [10:00<04:05, 81.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████▏                       | 8/10 [11:12<02:37, 78.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████            | 9/10 [12:21<01:15, 75.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [13:34<00:00, 81.48s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_eval_results = evaluate_generation(eval_questions[:10], query_engine, relevancy_evaluator, faithfulness_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e88e1609-c5a5-46ed-9e7f-4a0bdb166fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevancy</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  val\n",
       "0     Relevancy  0.7\n",
       "1  Faithfulness  0.9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_llm_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    metric_cols = ['Relevancy', 'Faithfulness']\n",
    "    metric_dicts = []\n",
    "    for metric_col in metric_cols:\n",
    "        metric_val = len(eval_results[eval_results[metric_col]])/len(eval_results)\n",
    "        metric_dict = [metric_col, metric_val]\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    metric_df = pd.DataFrame(metric_dicts, columns=['name', 'val'])\n",
    "    return metric_df\n",
    "\n",
    "display_llm_results('llm eval results', llm_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7649f-52f7-4bc6-aeac-6d5ef68bd17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
